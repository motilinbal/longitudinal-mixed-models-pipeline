## **Implementation Roadmap: A Lightweight Scripting Pipeline for Longitudinal Statistical Analysis**

## **Introduction**

This document outlines an implementation plan for a robust and reproducible statistical analysis pipeline. The objective is an exhaustive characterization of treatment effects from the longitudinal study.

This roadmap adopts a **lightweight, script-centric approach**. The goal is to create a series of simple, standalone Python scripts that can be executed sequentially, allowing for manual control and intuitive, REPL-style (Read-Evaluate-Print Loop) interaction.

This methodology prioritizes **simplicity, transparency, and direct control** over complex automation. We will eschew heavy frameworks (like `cookiecutter-data-science`, `great-expectations`, and `Makefile` orchestration) in favor of straightforward, numbered scripts. The full statistical integrity standard (e.g., LMM/GLMM, MICE, post-hoc correction) is well established; only the implementation philosophy is kept simple.

This roadmap serves as the definitive blueprint for the development team.

-----

## **I. Foundation: Project Architecture and Environment**

### **1.1 Simple Project Structure**

Instead of a complex template, we will use a minimal, logical directory structure. This flat structure is intuitive and easy to navigate. Create the following directories at the project root:

```
longitudinal_analysis/
│
├── data/
│   ├── raw/
│   │   └── (Original, read-only CSV files go here)
│   └── processed/
│       └── (All intermediate and final data files)
│
├── scripts/
│   ├── 01_load_and_validate.py
│   ├── 02_handle_outliers.py
│   ├── 03_impute_missing_data.py
│   ├── 04_generate_eda_report.ipynb
│   ├── 05_power_analysis.py
│   ├── 06_run_batch_modeling.py
│   ├── 07_run_batch_diagnostics.py
│   ├── 08_extract_and_correct_results.py
│   ├── 09_run_sensitivity_analysis.py
│   ├── 10_report_template.qmd
│   └── 11_run_batch_reporting.py
│
└── reports/
    ├── figures/
    │   └── (All static plots: diagnostics, power, etc.)
    ├── tables/
    │   └── (Final CSV results tables)
    └── (Final .html reports generated by script 11)
```

**Execution Workflow:** The pipeline is executed by manually running the scripts in `scripts/` in their numbered order. This provides complete control and allows for inspection of intermediate files in `data/processed/` at each step.

### **1.2 Environment and Dependency Management**

Reproducibility still requires a locked environment. We will use **pip** for Python packages and **R** for R dependencies, with an automated setup script to handle the complete installation.

**`requirements.txt`:**

```
# Python Core
pandas>=1.5.0
polars>=0.19.0
scikit-learn>=1.3.0
statsmodels>=0.14.0
seaborn>=0.12.0
matplotlib>=3.7.0
openpyxl>=3.1.0
pyarrow>=12.0.0  # For parquet

# R & pymer4
rpy2>=3.5.0
pymer4==0.9.2

# Reporting
jupyter>=1.0.0
```

**Prerequisites:**
- Python 3.10+
- R 4.0+ (must be installed separately)

**Setup Command:**
```bash
python setup_environment.py
```

This automated script will:
1. Install all Python packages from requirements.txt
2. Install required R packages (lme4, lmerTest, emmeans, MASS)
3. Verify the installation and test imports

**Manual Installation (alternative):**
```bash
# Install Python packages
pip install -r requirements.txt

# Install R packages
Rscript -e "install.packages(c('lme4', 'lmerTest', 'emmeans', 'MASS'), repos='https://cran.r-project.org/')"
```

### **1.3 Tooling Rationale: The Hybrid Python/R Ecosystem**

We will retain the core statistical tooling. The hybrid **Python/R** approach using the **pymer4** library remains the best choice. It provides a Pythonic interface to R's gold-standard `lme4` and `lmerTest` packages, which is essential for our mixed-effects modeling.

### **1.4 Parameter Management**

Instead of a central `config.yml` file, we will adopt a simpler approach:

  * **Key parameters** (e.g., file paths, outcome variable lists, imputation numbers, alpha levels) will be defined as **global constants at the top of each script** that uses them.
  * This makes each script self-contained and easy to modify for "one-time" analysis without managing a separate configuration system.

-----

## **II. Phase 1: Data Ingestion and Validation**

### **Script 01: `scripts/01_load_and_validate.py`**

  * **Objective:** Load, consolidate, and validate the raw data.
  * **Implementation Guidance:**
    1.  Define file paths as constants at the top of the script.
    2.  Use `pandas` or `polars` to read and concatenate the four CSV files from `data/raw/`.
    3.  **Lightweight Validation:** Instead of `Great Expectations`, use simple `assert` statements directly in the script. This serves as a "poor man's" data quality gate and is highly effective for a REPL-driven workflow. If an assertion fails, the script will stop, alerting the user to the problem.
    4.  **Example Assertions:**
        ```python
        # Example validation checks
        assert df['participant_id'].notnull().all(), "Missing participant IDs"
        assert df['time_point'].isin(range(1, 9)).all(), "Invalid time_point value"
        assert df['treatment'].isin(['A', 'B']).all(), "Invalid treatment value"
        ```
    5.  The script will be written to be run top-to-bottom. **Do not** use `if __name__ == "__main__"` blocks.
  * **Inputs:** `data/raw/replicate_*.csv`
  * **Outputs:** `data/processed/01_validated_data.parquet`

-----

## **III. Phase 2: Preprocessing and Feature Engineering**

### **Script 02: `scripts/02_handle_outliers.py`**

  * **Objective:** Systematically detect and flag potential outliers.
  * **Implementation Guidance:**
    1.  Define the `contamination` hyperparameter (e.g., `CONTAMINATION = 0.05`) as a constant at the top.
    2.  Load `data/processed/01_validated_data.parquet`.
    3.  Use `sklearn.ensemble.IsolationForest`.
    4.  Fit the model and use `.predict()` to get outlier flags.
    5.  Add new boolean columns (e.g., `measurement_X_is_outlier`) to the DataFrame. **Do not remove outliers**; only flag them for later sensitivity analysis.
  * **Inputs:** `data/processed/01_validated_data.parquet`
  * **Outputs:** `data/processed/02_outliers_flagged.parquet`

### **Script 03: `scripts/03_impute_missing_data.py`**

  * **Objective:** Impute missing values using Multiple Imputation by Chained Equations (MICE).
  * **Statistical Justification:** MICE is retained as it is the correct statistical approach, accounting for imputation uncertainty.
  * **Implementation Guidance:**
    1.  Define the number of imputations (e.g., `M = 20`) as a constant at the top.
    2.  Load `data/processed/02_outliers_flagged.parquet`.
    3.  Use `statsmodels.imputation.mice`.
    4.  Generate and save all $m$ imputed datasets to a new subdirectory: `data/processed/mice_imputations/imputed_data_*.parquet`.
    5.  For the primary analysis path, also save the *first* imputed dataset as the main processed file.
  * **Inputs:** `data/processed/02_outliers_flagged.parquet`
  * **Outputs:** `data/processed/03_imputed_data.parquet` (this is dataset \#1), `data/processed/mice_imputations/imputed_data_*.parquet` (all *m* datasets).

-----

## **IV. Phase 3: Exploratory Data Analysis (EDA)**

### **Script 04: `scripts/04_generate_eda_report.ipynb`**

  * **Objective:** To generate a comprehensive EDA report. A Jupyter Notebook is the ideal REPL-driven tool for this task.
  * **Implementation Guidance:**
    1.  Load `data/processed/03_imputed_data.parquet`.
    2.  Use `seaborn` and `matplotlib` for all visualizations.
    3.  **Required Visualizations:**
          * Spaghetti Plots (per-participant trajectories).
          * Mean Trend Plots (treatment group averages $\pm$ 95% CI).
          * Distribution Plots (histograms/kde) for each outcome.
    4.  **Normality Assessment:** For each outcome, perform a **Shapiro-Wilk test** (`scipy.stats.shapiro`) on the residuals of a simple linear model.
    5.  Save the p-value for each outcome to a simple CSV. This file will control the model selection in Phase 5.
  * **Inputs:** `data/processed/03_imputed_data.parquet`
  * **Outputs:** `reports/eda_report.html` (exported from notebook), `data/processed/04_normality_assessment.csv`

-----

## **V. Phase 4: Power Analysis (Pre-Modeling)**

### **Script 05: `scripts/05_power_analysis.py`**

  * **Objective:** Conduct a post-hoc power analysis via simulation.
  * **Implementation Guidance:**
    1.  Load `data/processed/03_imputed_data.parquet`.
    2.  Fit the primary LMM to the real data to get realistic variance components.
    3.  Define a range of effect sizes to test (e.g., `EFFECT_SIZES = [0.1, 0.2, 0.5, ...]`).
    4.  Write a simple `for` loop to iterate through the effect sizes.
    5.  Inside, use a nested `for` loop (e.g., 1000 iterations) to:
        a.  Simulate new data using `pymer4.simulate` with the given effect size.
        b.  Refit the LMM to the simulated data.
        c.  Check if the p-value for the interaction is significant.
    6.  Calculate power as the proportion of significant simulations.
    7.  Use `matplotlib` to save a plot of the power curves.
  * **Inputs:** `data/processed/03_imputed_data.parquet`
  * **Outputs:** `reports/figures/power_analysis.png`

-----

## **VI. Phase 5: Core Longitudinal Modeling**

### **Script 06: `scripts/06_run_batch_modeling.py`**

  * **Objective:** To fit the LMM or GLMM for *every* outcome variable in a single, simple script. This script replaces scripts 06, 07, and 08 from the original plan.
  * **Implementation Guidance:**
    1.  Define the list of outcome variables as a constant: `OUTCOME_VARIABLES = ['outcome_1', 'outcome_2', ...]`.
    2.  Load `data/processed/03_imputed_data.parquet`.
    3.  Load `data/processed/04_normality_assessment.csv` into a dictionary for easy lookup.
    4.  Initialize an empty list or dictionary to log any convergence failures.
    5.  Start a **simple `for` loop** to iterate through `OUTCOME_VARIABLES`. **Do not use `joblib` or parallelization.**
    6.  **Inside the loop:**
        a.  Check the normality assessment for the current outcome.
        b.  Define the model formula: `formula = f"{outcome} ~ treatment * time_point + (1 + time_point | participant_id)"`.
        c.  Use a `try...except` block for robust fitting.
        d.  **If Normal (LMM):** Use `pymer4.models.lmer(formula, ...)`
        e.  **If Non-Normal (GLMM):** Use `pymer4.models.glmer(formula, family=...)`. This script must support 'binomial', 'poisson', and 'negative.binomial' families.
        f.  **Convergence Fallback:** If the maximal model fails (check `model.show_logs()`), log the failure and automatically try a simpler model (e.g., `... + (1 | participant_id)`).
        g.  Save the successfully fitted model object using `pickle` or `joblib.dump`.
  * **Inputs:** `data/processed/03_imputed_data.parquet`, `data/processed/04_normality_assessment.csv`
  * **Outputs:** Multiple model files in `models/` (e.g., `models/outcome_1_lmm.pkl`, `models/outcome_2_glmm.pkl`)

-----

## **VII. Phase 6: Model Diagnostics and Validation**

### **Script 07: `scripts/07_run_batch_diagnostics.py`**

  * **Objective:** To automatically generate diagnostic plots for all fitted models. This script replaces 09 and 10 from the original plan.
  * **Implementation Guidance:**
    1.  Use `pathlib` or `glob` to find all `.pkl` model files in the `models/` directory.
    2.  Start a **simple `for` loop** to iterate through the list of model files. **Do not use `joblib`**.
    3.  **Inside the loop:**
        a.  Load the fitted `pymer4` model object.
        b.  Access residuals (`model.data['.resid']`) and fitted values (`model.data['.fitted']`).
        c.  Generate standard diagnostic plots using `matplotlib` or `seaborn`:
        \* Residuals vs. Fitted Plot
        \* Normal Q-Q Plot of residuals
        \* Scale-Location Plot
        d.  Save the plots as a single `.png` file.
  * **Inputs:** All `.pkl` files in `models/`
  * **Outputs:** Multiple plot files in `reports/figures/` (e.g., `reports/figures/outcome_1_diagnostics.png`)

-----

## **VIII. Phase 7: Inference, Correction, and Synthesis**

### **Script 08: `scripts/08_extract_and_correct_results.py`**

  * **Objective:** Extract all model results, perform post-hoc tests, apply FDR correction, and save to a final table.
  * **Implementation Guidance:**
    1.  Again, get the list of all model files from the `models/` directory.
    2.  Initialize empty lists to store results (e.g., `overall_results = []`, `posthoc_results = []`).
    3.  Loop through each model file:
        a.  Load the model.
        b.  **Overall Interaction:** Extract the p-value for the `treatment:time_point` interaction from `model.summary()`. Store it.
        c.  **Post-Hoc Tests:** Use `model.emmeans()` to get estimated marginal means for the treatment effect at each of the 8 time points. Store these pairwise comparisons.
    4.  **FDR Correction:** After the loop, you will have two collections of p-values.
        a.  Apply **Benjamini-Hochberg (BH)** correction using `statsmodels.stats.multitest.fdrcorrection` to the *overall interaction p-values*.
        b.  Apply BH correction separately to the *full set of all post-hoc p-values*.
    5.  Convert the results lists to `pandas` DataFrames and save them as CSVs.
  * **Inputs:** All `.pkl` files in `models/`
  * **Outputs:** `reports/tables/final_corrected_results.csv`, `reports/tables/posthoc_corrected_results.csv`

-----

## **IX. Phase 8: Sensitivity Analysis**

### **Script 09: `scripts/09_run_sensitivity_analysis.py`**

  * **Objective:** To re-run the analysis under different assumptions to check robustness.
  * **Implementation Guidance:** This script will be a "meta-script" that re-implements the logic from scripts 06 and 08 in two modified ways.
    1.  **Outlier Exclusion Analysis:**
        a.  Load `data/processed/02_outliers_flagged.parquet`.
        b.  Create a new DataFrame *with outliers removed*.
        c.  Run a simplified version of the modeling loop from script 06 on this new data.
        d.  Run a simplified version of the extraction loop from script 08.
        e.  Save the results.
    2.  **Multiple Imputation Analysis:**
        a.  Get the list of all `m` imputed datasets from `data/processed/mice_imputations/`.
        b.  Loop $m$ times, once for each dataset. In each loop, fit the models (like script 06) and extract coefficients and standard errors (like script 08).
        c.  After the loop, pool the $m$ sets of results using **Rubin's Rules** (which can be implemented manually or using helpers from `statsmodels`).
        d.  Save the final pooled results.
  * **Inputs:** `data/processed/02_outliers_flagged.parquet`, `data/processed/mice_imputations/`
  * **Outputs:** `reports/tables/sensitivity_outlier_results.csv`, `reports/tables/sensitivity_mice_pooled_results.csv`

-----

## **X. Phase 9: Automated Report Generation**

### **Script 10: `scripts/10_report_template.qmd`**

  * **Objective:** To create a reusable, parameterized **Quarto** template. This is the correct tool for high-quality, reproducible reports.
  * **Implementation Guidance:**
    1.  The file will be a `.qmd` file combining Markdown text with executable Python code chunks.
    2.  **Parameterization:** The YAML header will declare an input parameter:
        ```yaml
        ---
        title: "Treatment Effect Analysis: {{< params.outcome_variable >}}"
        format: html
        params:
          outcome_variable: "DefaultOutcome"
        ---
        ```
    3.  **Report Structure:** The template will contain code chunks that:
        a.  Load the specific model file (e.g., `models/{params.outcome_variable}_lmm.pkl`).
        b.  Load the relevant diagnostic plot (`reports/figures/{params.outcome_variable}_diagnostics.png`).
        c.  Load the results from `reports/tables/final_corrected_results.csv` and filter for the current outcome.
        d.  Display tables and plots in a clean, narrative format.
        e.  Include a section for **Cohen's d** to report a standardized effect size.

### **Script 11: `scripts/11_run_batch_reporting.py`**

  * **Objective:** To programmatically render the Quarto template for each outcome variable.
  * **Implementation Guidance:**
    1.  Define the list of `OUTCOME_VARIABLES` at the top (same as in script 06).
    2.  Start a **simple `for` loop** over this list. **Do not use `joblib`**.
    3.  Inside the loop, use Python's `subprocess.run()` module to call the Quarto CLI:
        ```python
        import subprocess

        outcome = 'outcome_1' # From the loop

        subprocess.run([
            "quarto", "render", "scripts/10_report_template.qmd",
            "-P", f"outcome_variable:{outcome}",
            "-o", f"reports/{outcome}_final_report.html"
        ])
        ```
  * **Inputs:** `scripts/10_report_template.qmd`, all model/figure/table files.
  * **Outputs:** A set of final `.html` reports in the `reports/` directory.

-----

## **XI. Documentation**

Each script in the `scripts/` directory must include a brief docstring or comment block at the top explaining its **purpose, key inputs, and key outputs**. This ensures clarity for any analyst picking up the project.
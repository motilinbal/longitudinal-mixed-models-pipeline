## **Implementation Roadmap: A Reproducible Pipeline for Longitudinal Statistical Analysis**

## **I. Foundation: Project Architecture and Environment Setup**

This document outlines the definitive implementation plan for a scalable and reproducible statistical analysis pipeline. The objective is to construct an automated system for the exhaustive characterization of treatment effects in a complex longitudinal study. This roadmap serves as an actionable blueprint for the development team, detailing the project's architecture, environment, and a modular, script-based workflow. Adherence to the principles and specifications herein is critical for ensuring the final pipeline is robust, maintainable, and scientifically rigorous.

### **1.1 Standardized Project Structure**

The foundation of a reproducible analysis is a logical and standardized project architecture. To prevent a disorganized collection of scripts and data files, the project will be initialized using the cookiecutter-data-science template.4 Version 2 of this tool introduces a dedicated Python package and command-line interface (ccds), which streamlines project setup.5 This industry-standard framework provides a pre-defined, logical directory structure that programmatically enforces a separation of concerns, which is fundamental to a robust and maintainable workflow.7  
The key directories and their mandated purposes are as follows:

* data/raw/: This directory will house the original, immutable source data (e.g., the four replicate CSV files). This data is to be treated as read-only.  
* data/processed/: This directory will contain all intermediate and final datasets generated by the pipeline's scripts. No data should be placed here manually; every file must be a reproducible output of a script located in src/.  
* src/: This directory will contain all Python source code organized into subdirectories (data, features, models, visualization) as prescribed by the template.7 These scripts form the executable logic of the entire pipeline.  
* notebooks/: For exploratory, non-production analysis and prototyping. Production scripts will reside in src/.  
* reports/: This directory will contain all final outputs, including data validation reports, figures, tables, and the final parameterized analysis reports generated by Quarto.  
* models/: For storing serialized, trained statistical models (e.g., pickled pymer4 objects).  
* Makefile: This file will serve as the primary orchestration layer. It will define targets that chain the pipeline's scripts in the correct order (e.g., make data to run all preprocessing, make report to run the full pipeline). This transforms the project from a mere collection of scripts into a cohesive, engineered system that can be executed from a single command.7

To initialize this structure, the development team will first install the required package and then run the command-line tool 5:

Bash

\# Install the package via pip before running  
pip install cookiecutter-data-science

\# Run the command-line tool from the parent directory  
ccds

### **1.2 Environment and Dependency Management**

Reproducibility is contingent on a precisely defined and version-controlled computational environment. To this end, all dependencies, including the Python and R runtimes and their respective packages, will be managed using conda. conda is uniquely suited for this project due to its robust handling of both Python and non-Python dependencies.1  
A version-controlled environment.yml file will be the single source of truth for the project's environment. This file will specify:

* The exact Python version (\>=3.10, a requirement for pymer4 8).  
* The R runtime (r-base).  
* All required Python packages with pinned versions (e.g., pymer4==0.9.2, statsmodels, great-expectations, seaborn, pandas, polars, scikit-learn, quarto, joblib).  
* All required R packages, installed via the conda-forge channel (e.g., r-lme4, r-lmertest).

This approach ensures that any developer can perfectly replicate the necessary environment with a single command (conda env create \-f environment.yml), eliminating "it works on my machine" issues and guaranteeing long-term reproducibility.

### **1.3 Tooling Rationale: The Hybrid Python/R Ecosystem**

The selection of the core modeling tool is a critical architectural decision. While Python's ecosystem is mature for general data science, it lacks a formula-based, frequentist mixed-effects modeling library that matches the power and widespread acceptance of R's lme4 package.8  
Therefore, this pipeline will adopt a strategic hybrid approach by using the pymer4 library (version 0.9.2 or later).8 This library functions as a sophisticated facade, providing a clean, Pythonic API that internally uses rpy2 to call R's lme4 and lmerTest packages.9 This choice represents a sound architectural trade-off, granting access to the gold-standard statistical engine without sacrificing the benefits of a Python-centric workflow. The latest versions of pymer4 also include robust support for logistic models and other GLMMs, providing a clear path for future extensions of this analysis.11

### **1.4 Centralized Configuration Management**

To enhance tunability and maintainability, all key parameters for the pipeline will be centralized in a single YAML configuration file (e.g., src/config.yml). This avoids hardcoding values within scripts and allows for easy modification of the analysis without altering the source code.  
This configuration file will manage parameters such as:

* The list of outcome variables to be analyzed.  
* The number of imputations (m) for the MICE procedure.  
* The contamination parameter for the Isolation Forest outlier detection.  
* Alpha levels for statistical significance and FDR control.  
* File paths and naming conventions.

Each script will be responsible for loading this configuration file at runtime to retrieve its necessary parameters.

### **1.5 Continuous Integration and Delivery (CI/CD)**

To ensure code quality and the continuous validity of the analysis, a CI/CD pipeline will be established using GitHub Actions. A workflow file (e.g., .github/workflows/main.yml) will be configured to automatically trigger on every push to the main branch.13  
This workflow will execute key targets from the Makefile, such as:

* make install: To build the environment and install dependencies.  
* make test: To run all unit and integration tests.  
* make validate: A new target that runs the Great Expectations validation script (01\_load\_and\_validate.py) on a small, representative sample of data.

This automated process serves as a critical safety net, ensuring that any changes to the codebase do not break the pipeline or compromise data integrity.14

## **II. Phase 1: Data Ingestion and Validation**

This initial phase focuses on securely ingesting the raw data and establishing a robust, automated quality gate.

### **Script 01: src/data/01\_load\_and\_validate.py**

* **Objective:** To load, consolidate, and perform rigorous, automated data validation.  
* **Statistical Justification:** Automated validation serves as the first and most critical line of defense against data quality issues that can silently corrupt model results.  
* **Implementation Guidance:**  
  1. Utilize polars or pandas to read and concatenate the four CSV files from data/raw/.  
  2. Employ the Great Expectations (GX) library to validate the consolidated data.16  
  3. An ExpectationSuite will be defined in a version-controlled JSON file. For enhanced efficiency, leverage modern GX features like **ExpectAI** to streamline the initial creation of this suite by generating expectations from natural language prompts or data profiles.  
  4. The script will run the validation process. If it fails, the script must raise a DataValidationError exception, log the detailed HTML report from GX, and terminate the pipeline.  
* **Inputs:** data/raw/replicate\_\*.csv.  
* **Outputs:** data/processed/01\_validated\_data.parquet, reports/data\_validation\_report.html.  
* **Testing Focus:** Unit tests must confirm that the script correctly raises an exception with mock data that violates the ExpectationSuite. Integration tests within the CI/CD pipeline will confirm that the validation runs successfully against a sample of valid data.

## **III. Phase 2: Preprocessing and Feature Engineering**

This phase addresses common data imperfections and prepares the dataset for modeling.

### **Script 02: src/features/02\_handle\_outliers.py**

* **Objective:** To systematically detect and flag potential outliers.  
* **Statistical Justification:** Outliers can exert undue influence on linear models. Identifying these points is crucial for assessing model robustness.  
* **Implementation Guidance:**  
  1. Use an **Isolation Forest** (sklearn.ensemble.IsolationForest), a robust, non-parametric method suitable for data with complex distributions.17  
  2. The contamination hyperparameter will be loaded from the central config.yml file.  
  3. The script will only *flag* outliers in new boolean columns (e.g., measurement\_X\_is\_outlier), not remove them. This enables sensitivity analysis in a later step.  
* **Inputs:** data/processed/01\_validated\_data.parquet.  
* **Outputs:** data/processed/02\_outliers\_flagged.parquet.

### **Script 03: src/features/03\_impute\_missing\_data.py**

* **Objective:** To impute missing values using Multiple Imputation by Chained Equations (MICE).  
* **Statistical Justification:** MICE is a superior approach to simple imputation methods as it creates multiple complete datasets, accounting for the uncertainty of the imputation process and leading to more reliable final inferences.19  
* **Implementation Guidance:**  
  1. Use the statsmodels.imputation.mice module.20  
  2. The number of imputations, $m$, will be specified in config.yml.  
  3. The script will generate and save all $m$ imputed datasets to data/processed/mice\_imputations/. For the primary analysis path, it will also save the *first* of these datasets to data/processed/03\_imputed\_data.parquet. The remaining datasets will be used for sensitivity analysis.  
* **Inputs:** data/processed/02\_outliers\_flagged.parquet.  
* **Outputs:** data/processed/03\_imputed\_data.parquet, data/processed/mice\_imputations/imputed\_data\_\*.parquet.

## **IV. Phase 3: Exploratory Data Analysis (EDA)**

This phase builds an intuitive understanding of the data's structure to inform modeling choices.

### **Script 04: src/visualization/04\_generate\_eda\_report.ipynb**

* **Objective:** To generate a comprehensive EDA report as an executable Jupyter Notebook.  
* **Statistical Justification:** EDA is essential for visually inspecting data distributions, relationships, and longitudinal trends, which helps validate modeling assumptions.  
* **Implementation Guidance:**  
  1. Use seaborn and matplotlib for visualizations.21  
  2. **Required Visualizations for each key outcome:**  
     * Spaghetti Plots (seaborn.lineplot with units='participant\_id').  
     * Mean Trend Plots ($\\pm$ 95% CI).  
     * Distribution Plots (seaborn.displot).  
  3. **Normality Assessment:** For each outcome, the script will perform a **Shapiro-Wilk test** (scipy.stats.shapiro) to formally assess the normality of residuals from a simple linear model.23 The results (p-value) will be saved to a summary file. If the p-value is below a threshold defined in config.yml (e.g., 0.05), the outcome will be flagged as non-normal, automatically routing it to the GLMM script in the modeling phase.  
* **Inputs:** data/processed/03\_imputed\_data.parquet.  
* **Outputs:** reports/eda\_report.html, data/processed/04\_normality\_assessment.csv.

## **V. Phase 4: Power Analysis (Pre-Modeling)**

This phase assesses the statistical power of the planned analysis to detect meaningful effects.

### **Script 05: src/models/05\_power\_analysis.py**

* **Objective:** To conduct a post-hoc power analysis via simulation to determine the probability of detecting the specified treatment effect given the study's design parameters.  
* **Statistical Justification:** A power analysis is a critical component of a rigorous statistical plan, providing confidence that the study is sufficiently powered to detect scientifically relevant effects. Simulation-based power analysis is highly flexible and necessary for complex designs like mixed-effects models where closed-form solutions are unavailable.26  
* **Implementation Guidance:**  
  1. This script will use the pymer4.simulate function, which leverages the fitted model structure to generate new data under a specified set of fixed-effect coefficients.28  
  2. The process will be:  
     a. Fit the primary LMM to the actual data to obtain realistic variance components (random effects).  
     b. Define a range of plausible effect sizes for the treatment:time\_point interaction, guided by the initial methodology.  
     c. For each effect size, run a simulation loop (e.g., 1000 iterations). In each iteration, simulate a new dataset using pymer4.simulate with the specified effect size, refit the LMM to this new data, and record whether the p-value for the interaction term is significant.  
     d. The statistical power for that effect size is the proportion of simulations where the effect was significant.  
  3. The results will be saved as a plot showing power curves for different effect sizes.  
* **Inputs:** data/processed/03\_imputed\_data.parquet.  
* **Outputs:** reports/figures/power\_analysis.png.

## **VI. Phase 5: Core Longitudinal Modeling**

This phase represents the analytical heart of the pipeline, where the primary statistical models are specified and fitted.

### **Script 06: src/models/06\_fit\_lmm.py**

* **Objective:** To fit the primary Linear Mixed-Effects Model (LMM) for a single, specified outcome variable.  
* **Statistical Justification:** LMMs are the standard tool for analyzing longitudinal data, correctly modeling the non-independence of repeated measures by incorporating random effects to account for individual heterogeneity.2  
* **Implementation Guidance:**  
  1. Use the pymer4.models.lmer class.9  
  2. **Model Formula:** f"{outcome\_variable} \~ treatment \* time\_point \+ (1 \+ time\_point | participant\_id)". This maximal random effects structure models individual differences in both baseline and change over time.  
  3. **Robust Error Handling:** The model fitting call will be wrapped in a try...except block.  
     * The script will check for convergence warnings using model.show\_logs().30  
     * If a convergence failure occurs, it will be logged using Python's standard logging module. The script will then automatically attempt to fit a simpler model by removing the random slope (formula: ... \+ (1 | participant\_id)). This fallback mechanism enhances pipeline robustness.  
  4. **Edge Cases:** Given the small sample size (n=32), a note will be logged if convergence issues arise, suggesting this as a potential cause. If diagnostic plots (Phase 6\) indicate heteroscedasticity, this script can be modified to use robust standard errors if a suitable library or method is identified.  
* **Inputs:** data/processed/03\_imputed\_data.parquet, outcome\_variable.  
* **Outputs:** models/{outcome\_variable}\_lmm.pkl.

### **Script 07: src/models/07\_fit\_glmm\_extension.py**

* **Objective:** To provide a parallel modeling script for outcome variables that do not follow a normal distribution.  
* **Statistical Justification:** For non-continuous or bounded data, a Generalized Linear Mixed-Model (GLMM) is required to avoid violating the assumptions of a standard LMM.31  
* **Implementation Guidance:**  
  1. Use the pymer4.models.glmer class.30  
  2. The script will accept a family argument. It will support standard families like 'binomial' and 'poisson'.  
  3. Crucially, it will also support the **'negative.binomial'** family (via MASS::glm.nb in R's backend) to properly model over-dispersed count data, a common issue where the variance is greater than the mean.33  
* **Inputs:** data/processed/03\_imputed\_data.parquet, outcome\_variable, family.  
* **Outputs:** models/{outcome\_variable}\_glmm.pkl.

### **Script 08: src/models/08\_run\_batch\_modeling.py**

* **Objective:** To orchestrate the execution of the model fitting scripts for every outcome variable.  
* **Implementation Guidance:**  
  1. The script will read the list of outcomes from config.yml and the normality flags from data/processed/04\_normality\_assessment.csv.  
  2. Based on the normality flag, it will route each outcome to either 06\_fit\_lmm.py or 07\_fit\_glmm\_extension.py.  
  3. **Parallelization:** To improve efficiency for a large number of outcomes, this script will use a process pool executor from Python's concurrent.futures or the joblib library to run the model fitting scripts in parallel.36  
* **Inputs:** src/config.yml, data/processed/04\_normality\_assessment.csv.  
* **Outputs:** A collection of serialized model files (.pkl) in the models/ directory.

## **VII. Phase 6: Model Diagnostics and Validation**

This phase is dedicated to assessing the statistical validity of the fitted models.

### **Script 09: src/visualization/09\_generate\_diagnostic\_plots.py**

* **Objective:** For each fitted model, generate a standardized set of diagnostic plots.  
* **Statistical Justification:** Visual inspection of diagnostic plots is the primary method for detecting violations of key LMM assumptions, such as normality and homoscedasticity of residuals.38  
* **Implementation Guidance:**  
  1. Load the fitted pymer4 model object. The model's .data attribute contains fitted values (.fitted) and residuals (.resid), enabling plotting with standard Python libraries.43  
  2. The following plots will be generated:  
     * **Residuals vs. Fitted Plot:** To check for non-linearity and heteroscedasticity.  
     * **Normal Q-Q Plot:** To assess the normality of residuals.  
     * **Scale-Location Plot:** To check for homoscedasticity.  
* **Inputs:** models/{outcome\_variable}\_lmm.pkl.  
* **Outputs:** reports/figures/{outcome\_variable}\_diagnostics.png.

### **Script 10: src/models/10\_run\_batch\_diagnostics.py**

* **Objective:** To automate the generation of diagnostic plots for all fitted models.  
* **Implementation Guidance:** This orchestration script will scan the models/ directory and invoke 09\_generate\_diagnostic\_plots.py for each model file. For efficiency, it will use concurrent.futures or joblib to generate the plots in parallel.36

## **VIII. Phase 7: Inference, Correction, and Synthesis**

In this phase, results are extracted, corrected for multiple comparisons, and synthesized into a final summary.

### **Script 11: src/models/11\_extract\_and\_correct\_results.py**

* **Objective:** To extract fixed-effects, perform post-hoc tests, apply False Discovery Rate (FDR) correction, and save the consolidated results.  
* **Statistical Justification:** When performing multiple tests (across outcomes and within post-hoc comparisons), correction is necessary to control the rate of false positives. The **Benjamini-Hochberg (BH) procedure** controls the FDR, providing a good balance between controlling for false positives and maintaining statistical power.45  
* **Implementation Guidance:**  
  1. The script will loop through each fitted model file.  
  2. **Overall Interaction Effect:** It will first extract the coefficient, p-value, and statistics for the primary treatment:time\_point interaction term from model.summary().  
  3. **Post-Hoc Pairwise Comparisons:** To decompose a significant interaction, it will perform post-hoc tests using pymer4's built-in capabilities. The model.emmeans() method will be used to compute the estimated marginal means for the treatment effect at each of the 8 time\_point levels.28 The resulting pairwise comparisons will provide timepoint-specific tests of the treatment effect.  
  4. **FDR Correction:** The script will collect two sets of p-values: (1) from the overall interaction terms across all outcomes, and (2) from all pairwise post-hoc tests across all outcomes. The statsmodels.stats.multitest.fdrcorrection function will be applied separately to each set to calculate BH-adjusted p-values (q-values).48  
* **Inputs:** All .pkl files in the models/ directory.  
* **Outputs:** reports/tables/final\_corrected\_results.csv, reports/tables/posthoc\_corrected\_results.csv.

## **IX. Phase 8: Sensitivity Analysis**

This phase ensures the robustness of the primary findings by re-running the analysis under different assumptions.

### **Script 12: src/analysis/12\_run\_sensitivity\_analysis.py**

* **Objective:** To systematically re-evaluate the primary models on alternative datasets to check the stability of the conclusions.  
* **Statistical Justification:** Sensitivity analysis is a hallmark of a defensible statistical report. It demonstrates that the key findings are not overly dependent on specific data-handling decisions, such as the treatment of outliers or the specific imputations used for missing data.  
* **Implementation Guidance:**  
  1. This script will orchestrate two separate sensitivity analyses by re-running the batch modeling and inference pipeline (08\_run\_batch\_modeling.py and 11\_extract\_and\_correct\_results.py):  
     * **Outlier Exclusion:** The pipeline will be run on a version of the dataset where flagged outliers have been removed.  
     * **Multiple Imputations:** The pipeline will be run independently on each of the $m$ datasets generated by the MICE procedure.  
  2. **Pooling MICE Results:** For the multiple imputation analysis, the script will then pool the results (coefficients and standard errors) from the $m$ separate runs using **Rubin's Rules**.50 While pymer4 does not have a native pooling function, this can be implemented by manually extracting the required statistics from each model and applying the pooling formulas, potentially leveraging utilities from statsmodels for guidance.52  
  3. The final pooled results and the results from the outlier exclusion analysis will be saved to separate files. These will be compared against the primary results to confirm that the main conclusions are robust.  
* **Inputs:** data/processed/02\_outliers\_flagged.parquet, data/processed/mice\_imputations/.  
* **Outputs:** reports/tables/sensitivity\_outlier\_results.csv, reports/tables/sensitivity\_mice\_pooled\_results.csv.

## **X. Phase 9: Automated Report Generation**

The final phase communicates the results through automated, high-quality, and reproducible reports.

### **Script 13: src/visualization/13\_create\_parameterized\_report.qmd**

* **Objective:** To create a reusable, parameterized Quarto template for generating a complete report for a single outcome variable.  
* **Statistical Justification:** A parameterized template ensures that every report is generated using the exact same logic, guaranteeing consistency and eliminating manual copy-paste errors.54  
* **Implementation Guidance:**  
  1. The .qmd file will combine Markdown for prose with executable Python code chunks for dynamic content.56  
  2. **Parameterization:** The YAML header will declare an input parameter, outcome\_variable, using Quarto's params block.55  
  3. **Accessibility:** To ensure compliance with modern standards, the YAML header will enable Quarto 1.8's built-in accessibility checks using the axe option 58:  
     YAML  
     \---  
     title: "Treatment Effect Analysis: {{\< params.outcome\_variable \>}}"  
     format:   
       html:  
         accessibility: true  
     params:  
       outcome\_variable: "DefaultOutcome"  
     \---

  4. **Report Structure:** The template will include sections for EDA, model results (including the summary table and post-hoc tests), and diagnostic plots.  
  5. **Clinical Relevance:** To enhance interpretability, a Python code chunk will calculate and display an effect size metric, such as **Cohen's d**, for the treatment comparison at a key time point.61 This provides a standardized measure of the magnitude of the treatment effect.  
* **Inputs:** This is a template file, not a directly executable script.  
* **Outputs:** A defined, reusable report structure.

### **Script 14: src/reports/14\_run\_batch\_reporting.py**

* **Objective:** To programmatically render the Quarto template for each outcome variable.  
* **Implementation Guidance:**  
  1. This script will read the list of outcome variables from config.yml.  
  2. It will loop through the list, executing the quarto render command via a subprocess call for each outcome. It will pass the outcome name as a parameter using the \-P flag and specify a unique output file.55  
  3. **Parallelization:** For efficiency, this script will use concurrent.futures or joblib to render multiple reports in parallel, significantly reducing the total runtime.36  
* **Inputs:** src/visualization/13\_create\_parameterized\_report.qmd, src/config.yml.  
* **Outputs:** A complete set of final analysis reports in the reports/ directory.

## **XI. Documentation**

Each script within the src/ directory must include a comprehensive docstring explaining its purpose, inputs, outputs, and any key implementation details. A README.md file at the root of the src/ directory will provide an overview of the pipeline's structure and the execution order of the scripts.

#### **Works cited**

1. Cookiecutter Data Science \- conda install \- Anaconda, accessed October 22, 2025, [https://anaconda.org/conda-forge/cookiecutter-data-science](https://anaconda.org/conda-forge/cookiecutter-data-science)  
2. Linear Mixed Models — Statistics and Machine Learning in Python 0.8 documentation, accessed October 22, 2025, [https://duchesnay.github.io/pystatsml/statistics/lmm/lmm.html](https://duchesnay.github.io/pystatsml/statistics/lmm/lmm.html)  
3. Introduction to Linear Mixed-Effects Models \- GeeksforGeeks, accessed October 22, 2025, [https://www.geeksforgeeks.org/machine-learning/introduction-to-linear-mixed-effects-models/](https://www.geeksforgeeks.org/machine-learning/introduction-to-linear-mixed-effects-models/)  
4. Structuring Your Data Science Project: A Guide to the Cookiecutter Template | by Utsav Raj, accessed October 22, 2025, [https://medium.com/@utsavraj.ptn04/structuring-your-data-science-project-a-guide-to-the-cookiecutter-template-c55a0eeac10e](https://medium.com/@utsavraj.ptn04/structuring-your-data-science-project-a-guide-to-the-cookiecutter-template-c55a0eeac10e)  
5. Cookiecutter Data Science, accessed October 22, 2025, [https://cookiecutter-data-science.drivendata.org/](https://cookiecutter-data-science.drivendata.org/)  
6. drivendataorg/cookiecutter-data-science \- GitHub, accessed October 22, 2025, [https://github.com/drivendataorg/cookiecutter-data-science](https://github.com/drivendataorg/cookiecutter-data-science)  
7. How to Structure Your Data Science Projects Using Cookiecutter ..., accessed October 22, 2025, [https://medium.com/@taeefnajib/how-to-structure-your-data-science-projects-5d4e471970f5](https://medium.com/@taeefnajib/how-to-structure-your-data-science-projects-5d4e471970f5)  
8. pymer4 \- PyPI, accessed October 22, 2025, [https://pypi.org/project/pymer4/](https://pypi.org/project/pymer4/)  
9. ejolly/pymer4 \- the convenience of lme4 in python \- GitHub, accessed October 22, 2025, [https://github.com/ejolly/pymer4](https://github.com/ejolly/pymer4)  
10. pymer4 \- piwheels, accessed October 22, 2025, [https://www.piwheels.org/project/pymer4/](https://www.piwheels.org/project/pymer4/)  
11. What's New — Pymer4 \- Eshin Jolly, accessed October 22, 2025, [https://eshinjolly.com/pymer4/pages/new.html](https://eshinjolly.com/pymer4/pages/new.html)  
12. Releases · ejolly/pymer4 \- GitHub, accessed October 22, 2025, [https://github.com/ejolly/pymer4/releases](https://github.com/ejolly/pymer4/releases)  
13. Quickstart for GitHub Actions, accessed October 22, 2025, [https://docs.github.com/en/actions/get-started/quickstart](https://docs.github.com/en/actions/get-started/quickstart)  
14. GitHub Actions and MakeFile: A Hands-on Introduction \- DataCamp, accessed October 22, 2025, [https://www.datacamp.com/tutorial/makefile-github-actions-tutorial](https://www.datacamp.com/tutorial/makefile-github-actions-tutorial)  
15. My Journey from GitHub Actions Back to Makefiles | by Yunus Emre Ak | Sep, 2025 | Medium, accessed October 22, 2025, [https://medium.yemreak.com/my-journey-from-github-actions-back-to-makefiles-63784e6c0da9](https://medium.yemreak.com/my-journey-from-github-actions-back-to-makefiles-63784e6c0da9)  
16. Great Expectations Tutorial: Validating Data with Python | DataCamp, accessed October 22, 2025, [https://www.datacamp.com/tutorial/great-expectations-tutorial](https://www.datacamp.com/tutorial/great-expectations-tutorial)  
17. A Guide to Outlier Detection in Python | Built In, accessed October 22, 2025, [https://builtin.com/data-science/outlier-detection-python](https://builtin.com/data-science/outlier-detection-python)  
18. 2.7. Novelty and Outlier Detection \- Scikit-learn, accessed October 22, 2025, [https://scikit-learn.org/stable/modules/outlier\_detection.html](https://scikit-learn.org/stable/modules/outlier_detection.html)  
19. Multivariate Imputation by Chained Equations (MICE) \- Kaggle, accessed October 22, 2025, [https://www.kaggle.com/discussions/general/556497](https://www.kaggle.com/discussions/general/556497)  
20. statsmodels.imputation.mice \- statsmodels 0.15.0 (+706), accessed October 22, 2025, [https://www.statsmodels.org/dev/\_modules/statsmodels/imputation/mice.html](https://www.statsmodels.org/dev/_modules/statsmodels/imputation/mice.html)  
21. Data structures accepted by seaborn \- PyData |, accessed October 22, 2025, [https://seaborn.pydata.org/tutorial/data\_structure.html](https://seaborn.pydata.org/tutorial/data_structure.html)  
22. seaborn: statistical data visualization — seaborn 0.13.2 documentation, accessed October 22, 2025, [https://seaborn.pydata.org/](https://seaborn.pydata.org/)  
23. Statistical tests for normality | Python, accessed October 22, 2025, [https://campus.datacamp.com/courses/introduction-to-portfolio-risk-management-in-python/univariate-investment-risk-and-returns?ex=12](https://campus.datacamp.com/courses/introduction-to-portfolio-risk-management-in-python/univariate-investment-risk-and-returns?ex=12)  
24. shapiro — SciPy v1.16.2 Manual, accessed October 22, 2025, [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html)  
25. How to Perform a Shapiro-Wilk Test in Python \- Statology, accessed October 22, 2025, [https://www.statology.org/shapiro-wilk-test-python/](https://www.statology.org/shapiro-wilk-test-python/)  
26. Simulation for Power Analysis \- GitHub Pages, accessed October 22, 2025, [https://nickch-k.github.io/EconometricsSlides/Week\_08/Power\_Simulations.html](https://nickch-k.github.io/EconometricsSlides/Week_08/Power_Simulations.html)  
27. Power Analysis by Simulation using R and simglm \- Iowa Research Online, accessed October 22, 2025, [https://iro.uiowa.edu/view/pdfCoverPage?instCode=01IOWA\_INST\&filePid=13675101040002771\&download=true](https://iro.uiowa.edu/view/pdfCoverPage?instCode=01IOWA_INST&filePid=13675101040002771&download=true)  
28. pymer4.models.lmer \- Eshin Jolly, accessed October 22, 2025, [https://eshinjolly.com/pymer4/api/models/lmer.html](https://eshinjolly.com/pymer4/api/models/lmer.html)  
29. Statistical Significance Testing using Linear Mixed Effect Models, accessed October 22, 2025, [https://www.cl.uni-heidelberg.de/statnlpgroup/empirical\_methods/kreutzer\_significance-python.html](https://www.cl.uni-heidelberg.de/statnlpgroup/empirical_methods/kreutzer_significance-python.html)  
30. Pymer4: Generalized Linear & Multi-level Models in Python — Pymer4, accessed October 22, 2025, [https://eshinjolly.com/pymer4/](https://eshinjolly.com/pymer4/)  
31. Generalized Linear & Linear Mixed Models — Pymer4 \- Eshin Jolly, accessed October 22, 2025, [https://eshinjolly.com/pymer4/tutorials/04\_glmms.html](https://eshinjolly.com/pymer4/tutorials/04_glmms.html)  
32. Generalized Linear Mixed Effects Models in R and Python with GPBoost \- Medium, accessed October 22, 2025, [https://medium.com/data-science/generalized-linear-mixed-effects-models-in-r-and-python-with-gpboost-89297622820c](https://medium.com/data-science/generalized-linear-mixed-effects-models-in-r-and-python-with-gpboost-89297622820c)  
33. Getting Started with Negative Binomial Regression Modeling \- UVA Library, accessed October 22, 2025, [https://library.virginia.edu/data/articles/getting-started-with-negative-binomial-regression-modeling](https://library.virginia.edu/data/articles/getting-started-with-negative-binomial-regression-modeling)  
34. Negative Binomial Regression | R Data Analysis Examples \- OARC Stats \- UCLA, accessed October 22, 2025, [https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/](https://stats.oarc.ucla.edu/r/dae/negative-binomial-regression/)  
35. Fitting Negative Binomial GLMMs \- R, accessed October 22, 2025, [https://search.r-project.org/CRAN/refmans/lme4/help/glmer.nb.html](https://search.r-project.org/CRAN/refmans/lme4/help/glmer.nb.html)  
36. Performance: joblib.Parallel is significantly slower than ProcessPoolExecutor for tasks with large objects \#1733 \- GitHub, accessed October 22, 2025, [https://github.com/joblib/joblib/issues/1733](https://github.com/joblib/joblib/issues/1733)  
37. How frequently do you use parallel processing at work? : r/Python \- Reddit, accessed October 22, 2025, [https://www.reddit.com/r/Python/comments/1ii1i6z/how\_frequently\_do\_you\_use\_parallel\_processing\_at/](https://www.reddit.com/r/Python/comments/1ii1i6z/how_frequently_do_you_use_parallel_processing_at/)  
38. How to Generate Diagnostic Plots with statsmodels for Regression Models \- Statology, accessed October 22, 2025, [https://www.statology.org/how-to-generate-diagnostic-plots-with-statsmodels-for-regression-models/](https://www.statology.org/how-to-generate-diagnostic-plots-with-statsmodels-for-regression-models/)  
39. Understanding Diagnostic Plots for Linear Regression Analysis \- UVA Library, accessed October 22, 2025, [https://library.virginia.edu/data/articles/diagnostic-plots](https://library.virginia.edu/data/articles/diagnostic-plots)  
40. Creating Diagnostic Plots in Python \- Robert Alvarez, accessed October 22, 2025, [https://robert-alvarez.github.io/2018-06-04-diagnostic\_plots/](https://robert-alvarez.github.io/2018-06-04-diagnostic_plots/)  
41. Mixed Models: Diagnostics and Inference \- Social Science Computing Cooperative, accessed October 22, 2025, [https://sscc.wisc.edu/sscc/pubs/MM/MM\_DiagInfer.html](https://sscc.wisc.edu/sscc/pubs/MM/MM_DiagInfer.html)  
42. Mixed Model Diagnostics \- Cloudfront.net, accessed October 22, 2025, [https://dfzljdn9uc3pi.cloudfront.net/2020/9522/1/MixedModelDiagnostics.html](https://dfzljdn9uc3pi.cloudfront.net/2020/9522/1/MixedModelDiagnostics.html)  
43. Coming from R — Pymer4 \- Eshin Jolly, accessed October 22, 2025, [https://eshinjolly.com/pymer4/pages/comingfromR.html](https://eshinjolly.com/pymer4/pages/comingfromR.html)  
44. Quickstart — Pymer4 \- Eshin Jolly, accessed October 22, 2025, [https://eshinjolly.com/pymer4/pages/quickstart.html](https://eshinjolly.com/pymer4/pages/quickstart.html)  
45. Benjamini-Hochberg Procedure \- GeeksforGeeks, accessed October 22, 2025, [https://www.geeksforgeeks.org/data-science/benjamini-hochberg-procedure/](https://www.geeksforgeeks.org/data-science/benjamini-hochberg-procedure/)  
46. Multiple Testing — Introduction to Statistical Learning (Python) \- ISLP documentation\!, accessed October 22, 2025, [https://islp.readthedocs.io/en/latest/labs/Ch13-multiple-lab.html](https://islp.readthedocs.io/en/latest/labs/Ch13-multiple-lab.html)  
47. pymer4.models.lm \- Eshin Jolly, accessed October 22, 2025, [https://eshinjolly.com/pymer4/api/models/lm.html](https://eshinjolly.com/pymer4/api/models/lm.html)  
48. false\_discovery\_control — SciPy v1.16.2 Manual, accessed October 22, 2025, [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.false\_discovery\_control.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.false_discovery_control.html)  
49. statsmodels.stats.multitest.fdrcorrection, accessed October 22, 2025, [https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.fdrcorrection.html](https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.fdrcorrection.html)  
50. Combine estimates by pooling rules \- mice \- amices, accessed October 22, 2025, [https://amices.org/mice/reference/pool.html](https://amices.org/mice/reference/pool.html)  
51. 1.4 Multiple imputation in a nutshell \- Stef van Buuren, accessed October 22, 2025, [https://stefvanbuuren.name/fimd/sec-nutshell.html](https://stefvanbuuren.name/fimd/sec-nutshell.html)  
52. Source code for statsmodels.imputation.mice, accessed October 22, 2025, [https://www.statsmodels.org/stable/\_modules/statsmodels/imputation/mice.html](https://www.statsmodels.org/stable/_modules/statsmodels/imputation/mice.html)  
53. Analysis Models — Autoimpute documentation \- Read the Docs, accessed October 22, 2025, [https://autoimpute.readthedocs.io/en/latest/user\_guide/analysis.html](https://autoimpute.readthedocs.io/en/latest/user_guide/analysis.html)  
54. Using Python \- Quarto, accessed October 22, 2025, [https://quarto.org/docs/computations/python.html](https://quarto.org/docs/computations/python.html)  
55. From One Notebook to Many Reports: Parameterized reports with the jupyter engine, accessed October 22, 2025, [https://quarto.org/docs/blog/posts/2025-07-24-parameterized-reports-python/index.html](https://quarto.org/docs/blog/posts/2025-07-24-parameterized-reports-python/index.html)  
56. 31\. Quarto — Python for Data Science, accessed October 22, 2025, [https://aeturrell.github.io/python4DS/quarto.html](https://aeturrell.github.io/python4DS/quarto.html)  
57. How to Generate Analytics Reports in Python with Quarto | by Kevin Meneses González, accessed October 22, 2025, [https://medium.com/@kevinmenesesgonzalez/how-to-generate-analytics-reports-in-python-with-quarto-5dce5dd94b66](https://medium.com/@kevinmenesesgonzalez/how-to-generate-analytics-reports-in-python-with-quarto-5dce5dd94b66)  
58. Download Quarto, accessed October 22, 2025, [https://quarto.org/docs/download/](https://quarto.org/docs/download/)  
59. Quarto 1.8, accessed October 22, 2025, [https://quarto.org/docs/blog/posts/2025-10-13-1.8-release/](https://quarto.org/docs/blog/posts/2025-10-13-1.8-release/)  
60. HTML Accessibility Checks \- Quarto, accessed October 22, 2025, [https://quarto.org/docs/output-formats/html-accessibility.html](https://quarto.org/docs/output-formats/html-accessibility.html)  
61. How to calculate cohen's d in Python? \- Stack Overflow, accessed October 22, 2025, [https://stackoverflow.com/questions/21532471/how-to-calculate-cohens-d-in-python](https://stackoverflow.com/questions/21532471/how-to-calculate-cohens-d-in-python)  
62. What is Cohen's D in Python? \- AskPython, accessed October 22, 2025, [https://www.askpython.com/python/examples/cohens-d-python](https://www.askpython.com/python/examples/cohens-d-python)